\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{breqn}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
% \setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Mask inpainting with a GAN network}

\author{Luca Lumetti\\
{\tt\small 244577@studenti.unimore.it}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Federico Silvestri\\
{\tt\small 243938@studenti.unimore.it}
\and
Matteo Di Bartolomeo\\
{\tt\small 241469@studenti.unimore.it}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT

\begin{abstract}
  Our project aims to remove the surgical mask over a person's face by
  reconstructing the covered region. To reach our goal, we used a deep network
  to detect landmarks over the face, a classical pipeline to segment the
  surgical mask and produce as output a binary mask. Both original image and
  mask are then fed through the network which will output the reconstructed
  face. It is also possible to have a second photo of the same person without
  the surgical mask that can be used as a reference. From this photo, a small
  region of the cheek and mouth is extracted and warped over the original image,
  the mask is then adjusted to match the new image. This reference photo should
  provide to the network more information about the face thus leading to a more
  loyal reconstruction. The source code of the whole project is available at
  https://github.com/LucaLumetti/CVProject/
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Related works}
A lot of works on surgical mask segmentation have been made over the last year, since
the start of the Covid-19 pandemic, but all of them were completely based only on
end-to-end deep neural networks. About the task of image inpainting, a lot of
research has been done. Most of the works were about natural images inpainting,
\cite{yu2019free} \cite{li2020deepgin}, and facial
inpainting \cite{iizuka2017globally} \cite{li2017generative}
\cite{li2020learning}. The mask can be restricted to a fixed form (usually a
square) or be a free-form without any particular constraint.\\

\section{Mask Segmentation}
We made use of MediaPipe's FaceMesh \cite{DBLP:journals/corr/abs-1907-06724}
library to find facial landmarks over the face covered with the surgical mask
and the reference photo. Facial landmarks are important to have an initial
approximation of the region where to search the surgical mask and to warp the
reference photo over the first one. To perform the segmentation of the mask we
apply a k-means with k=3 over the polygon we created using specific face
landmarks and pick the bigger region between the 3.  The k has been chosen to
be 3 as in the polygonal region we expect to find the mask, the background and
the skin of the person's face. In the end, a binary image is created, with a 1
where the mask is present and 0 elsewhere, while in the original image, the mask
area is filled with 0s.
\begin{figure}
  \caption{Frontal image with landmarks on the left, resulting mask of the
  surgical mask on the right.}
  \includegraphics[width=0.49\linewidth]{img/landmarks_front.jpeg}
  \includegraphics[width=0.49\linewidth]{img/dilated.jpeg}
\end{figure}

\section{Warping the reference photo}
The objective of the reference photo is to guide the network to a more loyal
reconstruction, by giving some information on how the mouth and close parts
should look. As we allow the reference to have a yawn different than frontal, we
apply a thin-plate spline transformation to adjust it. We use 30 specific
landmarks as parameters as using more of them lead to distortions given by the
errors in the landmarks detection while less lead to an imperfect warping. The
same polygon region of Mask Segmentation is cut from the reference photo, then by
applying the TPS it is stuck to main photo leading to a (partial)
reconstruction. The image will then be fed to the network which will
reconstruct the missing parts.
\begin{figure}
  \caption{Reference photo with landmarks found by mediapipe on the left,
  resulting image on the right.}
  \includegraphics[width=0.49\linewidth]{img/landmarks_lateral.jpeg}
  \includegraphics[width=0.49\linewidth]{img/result.jpg}
\end{figure}

\section{Image inpainting}
Image inpainting is the task to fill a missing region
in an image by predicting the value of the missing pixels in order to have a
realistic image which is semantically close to the original one and visually
correct. There are two different approaches to achieve this task:
\begin{enumerate}
  \item Low-level feature patch-matching which does not work pretty well with
    non-stationary use-cases (e.g. faces, objects or complicate scenes);
  \item	Feed-forward models with deep convolutional networks which overcome the
    problem of previous case exploiting semantics learned on large scale
    dataset.
\end{enumerate}
\section{Our approach}
We decided to follow the latter one designing a coarse-to-fine Generative
Adversarial Network (GAN) characterized by:
\begin{itemize}
	\item Generator is made of a coarse network, whose aim is to provide a rough
    estimation of the missing region, and a refinement network which takes the
    output of the previous network and the binary mask as input and improve the
    coarse output by making it more detailed.
  \item	Discriminator which is responsible of distinguishing real samples from
    the one created by the generator.
\end{itemize}
The input of the network is a pre-processed RGB image so that its values are in
range \([-1,+1]\) and its binary mask. For initializing the starting values of
the network weights, we opted for Kaiming initialization method. We provided
different methods to initialize the starting values of the network weights, such
as normal, Xavier, orthogonal and, the default one, Kaiming. We used Adam as the
generator and discriminator optimizer using a \(0.5\) momentum and different
learning rates, respectively \(0.0001\) and \(0.0004\) for 10 epochs.

\subsection{Losses}
Our final loss function is given by the sum of six different losses:

\begin{dmath}
      \mathcal{L}_{tot} = \mathcal{L}_{adv} + \mathcal{L}_{recon} +
      \mathcal{L}_{tv} + \mathcal{L}_{contra} + \lambda_{perc} \cdot
      \mathcal{L}_{perc} + \lambda_{style} \cdot \mathcal{L}_{style}
\end{dmath}

Specifically, the weights $\lambda$ used are: $\lambda_{perc} = 0.05$,
$\lambda_{style} = 80$ and $\lambda_{adv} = 0.1$.\\
In the following formulas we will use symbols to refer to specific elements of
the loss function, $\mathbf{I}_{in}$ is the masked image, $\mathbf{I}_{gt}$ is
the reference ground truth image, $\mathbf{I}_{out}$ is the output of the
refinement stage, $\mathbf{I}_{com}$ is the masked image where masked pixels are
replaced with $\mathbf{I}_{out}$. \\

\subsubsection{Adversarial Loss.}
\begin{dmath}
    \mathcal{L}_{gen} = -\mathbb{E}_{\mathbf{I}_{in} \sim \mathbb{P}_i}
    [D(\mathbf{I}_{in},\mathbf{I}_{com})]
\end{dmath}
\begin{dmath}
    \mathcal{L}_{discr} = \mathbb{E}_{\mathbf{I}_{in} \sim \mathbb{P}_i}
    [\mathrm{ReLU}(1-D(\mathbf{I}_{in},\mathbf{I}_{gt}) +
    \mathrm{ReLU}(1+D(\mathbf{I}_{in},\mathbf{I}_{com})]
\end{dmath}
where \(\mathbb{P}_i\) is the data distribution of $\mathbf{I}_{in}$, \textit{D}
and \textit{G} are, respectively, the discriminator and the generator and ReLU
is the rectified linear unit defined as \(f(x) = \mathrm{max}(0,x)\).\\ This is
also know as GAN Hinge loss for generative adversarial learning, where
discriminator is trained to distinguish $\mathbf{I}_{com}$ from $\mathbf{I}_{gt}$ and generator
has the aim of cheating the classification of the discriminator.

\subsubsection{Reconstruction Loss.}
\begin{dmath}
  \mathcal{L}_{recon} = |\mathbf{I}_{gt} - \mathbf{I}_{coarse}| + |\mathbf{I}_{gt} - \mathbf{I}_{com}|
\end{dmath}
Is the standard L1 loss applied on both coarse and refined images.

\subsubsection{Total variation (TV) Loss.}
\begin{dmath}
    \mathcal{L}_{tv} = \sum^{H-1,W}_{x,y} \frac{\norm{ \mathbf{I}^{x+1,y}_{com}
    - \mathbf{I}^{x,y}_{com} }^2}{N^{row}_{\mathbf{I}_{com}}} +
    \sum^{H,W-1}_{x,y} \frac{\norm{ \mathbf{I}^{x,y+1}_{com} -
    \mathbf{I}^{x,y}_{com} }^2}{N^{col}_{\mathbf{I}_{com}}}
\end{dmath}
where \textit{H} and \textit{W} are the height and width of $\mathbf{I}_{com}$
and $N^{row}_{\mathbf{I}_{com}}$ and $N^{col}_{\mathbf{I}_{com}}$ are the number
of pixels in $\mathbf{I}_{com}$ without the last row and the last column.
\\
It is responsible for the regularization of the image to improve the smoothness
of the output image.

\subsubsection{Perceptual Loss.}
\begin{dmath}
    \mathcal{L}_{perceptual} = \sum^L_{l=1} \frac{\norm{
      \phi^{\mathbf{I}_{com}}_l - \phi^{\mathbf{I}_{gt}}_l
    }_1}{N_{\phi^{\mathbf{I}_{gt}}_l}}
\end{dmath}
where \(\phi\) is the well-trained loss network, VGG-19\cite{simonyan2014very},
and \(\phi^{\mathbf{I}}_l\) the activation maps of the \(l^{th}\) layer of
\(\phi\) given an image \(\mathbf{I}\).
\(N_{\phi^{\mathbf{I}_{gt}}_l}\) denotes the number of elements in
\(\phi^{\mathbf{I}_{gt}}_l\) and \textit{L} is the number of layers used.
This loss represents the L1-norm distance between high-level feature
representations in 5 different convolutive layers.

\subsubsection{Style Loss.}
\begin{dmath}
        \mathcal{L}_{style} = \sum^L_{l=1}\norm{ \frac{1}{C_l H_l W_l}
        ((\phi^{\mathbf{I}_{com}}_l)^\top(\phi^{\mathbf{I}_{com}}_l) -
        (\phi^{\mathbf{I}_{gt}}_l)^\top(\phi^{\mathbf{I}_{gt}}_l)  )}
\end{dmath}
where \(C_l\) refers to the number of activation maps of the \(l^{th}\) layer of
\(\phi\) and \(H_l\) and \(W_l\) are its height and width respectively. With
\((\phi^{\mathbf{I}}_l)^\top(\phi^{\mathbf{I}}_l)\) we represented the
auto-correlation matrix, the Gram matrix\cite{gatys1508neural} which computes
the features correlations between each activation map of the \(l^{th}\) layer of
\(\phi\) given the image \(\mathbf{I}\).\\
Using the same 5 levels of the previous loss, it is the sum of the distances of
the auto-correlation matrixes between the output and the ground truth multiplied
by a factor that depends on the size and number of the activation maps in those
layers.

\subsubsection{Contrastive loss.}
\begin{dmath}\label{eq:contra}
    \mathcal{L}_{contrastive} = - \log{\frac{exp(z_{i}^\intercal
    z_{i}^{'}/\tau)}{\sum_{j=0}^{K}exp(z_{i}z_{j}^{'}/\tau)}}
\end{dmath}
The equation \eqref{eq:contra} is the categorical cross-entropy of classifying
the positive sample correctly \cite{oord2018representation}. $z_i$ is the
encoded version of the image, \(\tau\) is a hyper-parameter that controls the
sensitivity of the product and it's
called temperature. The dot product between the encoding vector of the original
image and the transformated image measure the similarity between representations.
In our network the positive images are created by the original images with some
transformations. This methods try to maximize the similarity between
representations of positive similar pairs and minimize the similarity with the
feature extracted from negative images\cite{le2020contrastive}.
To calculate this loss we use the feature vector with the biggest number of
channels in our network (512). We squeeze this vector in a way that preserves the
information of the original image with average pooling so we use a vector with
dimension 1x1x512. The transformations used are inspired by
StyleGAN\cite{karras2020analyzing} and they are:
\begin{itemize}
  \item horizontal flip with probability 1.0
  \item change in brightness with probability 0.75
  \item change in contrast with probability 0.75
  \item change in saturation with probability 0.75
  \item hue rotation with probability 1.0
\end{itemize}
This loss and these transformations are been proven to be really effective in
our case as the network was overfitting the skin color, the mouth type and its
position.

\subsection{Datasets}
GAN networks are data-hungry and need a lot of diverse training examples in
order to generate quality images, for this reason we used the FFHQ 1024x1024
images \cite{karras2019style}, rescaled to 256x256.  In other GAN inpainting
architectures, the mask region to reconstruct is usually calculated during the
training in a randomized way.  As we do not need this randomization process, for
each image of FFHQ we precalculated the face region where the mask is worn
using facial landmarks. To test our network, instead, we use CelebA256 dataset.

\subsection{Architecture}
Our architecture is inspired by Free Form Image Inpainting with Gated
Convolution \cite{yu2019free} and DeepGIN \cite{li2020deepgin}.
A mixed-precision training is used to train the network, to be more precise,
\textbf{O2} option of nvidia-apex
(https://nvidia.github.io/apex/amp.html\#o2-almost-fp16-mixed-precision) has
been used on both discriminator and generator.
Our generator net is composed of two stages: Coarse Network and Refine Network.
\begin{figure}
  \includegraphics[width=1\linewidth]{img/generator.png}
  \caption{Generator network}
  \label{fig:generator}
\end{figure}
\begin{figure}
  \includegraphics[width=1\linewidth]{img/discriminator.png}
  \caption{Discriminator network, GAN loss is calculated on every patch of the
  last vector}
  \label{fig:discriminator}
\end{figure}
\subsubsection{Coarse Network}
In this stage we decided to use the gated convolution \cite{yu2019free} so that the generator is
able to learn a dynamic feature selection mechanism for each channel and for
each spatial location. The feature selection mechanism takes into account not
only the background and the mask given in input, but also the semantic
segmentation in some channels.
\\
As shown in figure \ref{fig:generator} the coarse net is
characterized by an initial downsampling phase, followed by a residual one
and at the end there is an upsampling phase using the dilated gated convolution
that could be seen as a gated convolution operation preceded by a resize
operation. The output of the coarse net will be multiplied with the mask and
added to the original image, then fed in the refine net.
\subsubsection{Refine Network}
The refine network takes the output of the coarse net and the mask as input.
In this stage there are 6 custom ResNet blocks with four different dilation rates and some
gated convolutional layers.
This modified ResNet blocks are called Spatial Pyramid Dilation (SPD) (Figure
\ref{fig:rnspd}). This layer is composed of different convolutional blocks with
different dilation, and the output of these blocks is concatenated together.
With different value of dilation rate we can take information from a bigger
receptive field.
\begin{figure}
  \includegraphics[width=1\linewidth]{img/SPDresnet.png}
  \caption{ResNetSPD block with 8 different dilation rate used in the coarse
  network}
  \label{fig:rnspd}
\end{figure}
Another useful feature that is implemented in this net is the Multi Scale Self
Attention (MSSA). The MSSA uses the self-similarity with the image itself
and it's helpful to have better coherence in the final image. We control
the self-similarity with three different scales: 16x16, 32x32, 64x64.
The central layers are composed of self attention block. We use a standard
convolutional layer to reduce the size before the self attention. In this manner
we avoid an excessive increase of the parameters. With self attention we can
find a better correlation between features and have a better reconstruction.
\subsubsection{Discriminator}
Our discriminator is composed of 6 convolutional blocks with kernel size 5 and
stride 2.
These convolutional methods allow to capture the Markovian patches that
represents better contextual feature\cite{li2016precomputed}.
We add a spectral normalization to improve the training
stabilization\cite{miyato2018spectral}.
The input of this network is the image (real or fake) and the relative mask.
\begin{dmath}
        \mathcal{L}_{D^{sn}}= \mathbb{E}_{x\sim \mathbb{P}_{data(x)}} \left [
          ReLU(1-D^{sn}(x))) \right ] + \mathbb{E}_{z\sim \mathbb{P}_{data(x)}}
        \left [ ReLU(1+D^{sn}(G(z)))\right ]
\end{dmath}

where \(D^{sn}\) is the spectral-normalized discriminator and G is the generator
that create the image z.

\subsection{Multi-GPU and Multi-Node Training}
% Da rivedere
As GAN networks are heavy to train, so we decided to design the whole network to be
trainable on multi GPU and multi node architectures. Training it on a single K80
was infeasible as a batch size greater than 2 gives CUDA out of memory error.
We end up using 4 GPU K80 in parallel that let us to complete a full epoch in
under 24h (which is the time limit on AImageLab's servers). We also give support
for multi node because even if we were on the same node. We initially could use
at most 2 GPUs per account, so every account could be seen as a different node
thus we could break the GPUs limit. Anyway as our max GPU per user was
upgraded to 4, we didn't use that technique.

\subsection{Results}
To evaluate the results (Table \ref{tab:results}) we use different metrics:
PSNR, SSIM, LPIPS and FID. We conducted the test with CelebA 256 Dataset and
compared the results with different inpainting network. It must be noted that
our network is the only one who is restricted to inpaint a specific part of an
image of a human face, while others generalize to the whole image.
\begin{table}
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Method & PSNR & SSIM & FID & LPIPS \\
    \hline
    lizuka et al. \cite{iizuka2017globally} & 18.62 & 0.69 & n.d. & 0.51 \\
    SymmFCNet \cite{li2020learning}         & 27.81 & 0.97 & n.d. & 0.61 \\
    DeepFillv2 \cite{yu2019free}            & 29.70 & 0.72 & 75.56 & 0.29 \\
    DIP \cite{Ulyanov_2018_CVPR}            & 25.46 & 0.85 & n.d. & n.d. \\
    Ours                                    & 58.95 & 0.88 & 151.95 & 0.23\\
  \hline
  \end{tabular}
  \\
  \caption{Comparison with different network \textbf{on different datasets} on the same metrics}
  \label{tab:results}
\end{table}

It must be noted that these results were got by using different datasets, we
just wanted to have a rough estimation of our distance from the state-of-the-art
networks.
Results are not that bad, FID is pretty high and we think that this is due to
some artifacts that often appear in the generated regions.
In the Table \ref{tab:photos} we show some of the reconstructed images.
\begin{table}
  \begin{tabular}{cccc}
    \includegraphics[width=.2\linewidth]{samples/00045.jpg}&
    \includegraphics[width=.2\linewidth]{samples/00053.jpg}&
    \includegraphics[width=.2\linewidth]{samples/00099.jpg}&
    \includegraphics[width=.2\linewidth]{samples/00146.jpg}\\
    \includegraphics[width=.2\linewidth]{samples/00202.jpg}&
    \includegraphics[width=.2\linewidth]{samples/00298.jpg}&
    \includegraphics[width=.2\linewidth]{samples/00353.jpg}&
    \includegraphics[width=.2\linewidth]{samples/00708.jpg}\\
    \includegraphics[width=.2\linewidth]{samples/recon_600.png}&
    \includegraphics[width=.2\linewidth]{samples/recon_900.png}&
    \includegraphics[width=.2\linewidth]{samples/aug_recon_1600.png}&
    \includegraphics[width=.2\linewidth]{samples/aug_recon_200.png}\\
\end{tabular}
  \label{tab:photos}
  \caption{Some of the generated images, the latter two were augmented before
  feeding them to the network.}
\end{table}

\subsection{Conclusion}
Our results are not too far to the state-of-the-art and we think that they can
be improved by training the network for more time, with more data and a larger
variety of masks. Some of the networks we have compared had \~10x more epochs
and used a lot more training data (CelebA + FFHQ + StyleGAN). We couldn't afford
that as the time and computing resource were not feasible for us.\\
What make this network different from previous works is the specific task
(inpainting the surgical mask region) and the use of a contrastive loss to
prevent overfitting.

{\small
\bibliographystyle{plain}
\bibliography{cvproject}
}

\end{document}
